{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[idx].replace(\".tif\", \"_FGT.tif\"))\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 경로 설정\n",
    "image_dir = \"C:/Users/s_zmfldlwx/Desktop/2024-1학기/OSSP-1/팀 프로젝트/New_Sample/2.원천데이터/1.항공사진_Fine_512픽셀\"\n",
    "mask_dir = \"C:/Users/s_zmfldlwx/Desktop/2024-1학기/OSSP-1/팀 프로젝트/New_Sample/1.라벨링데이터/1.항공사진_Fine_512픽셀/1.Ground_Truth_Tiff\"\n",
    "\n",
    "# 데이터 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# 데이터셋과 데이터로더 생성\n",
    "dataset = SatelliteDataset(image_dir, mask_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        def CBR2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=True):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride, bias=bias),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        def ConvLayer2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=True):\n",
    "            return nn.Sequential(\n",
    "                CBR2d(in_channels, out_channels, kernel_size, padding, stride, bias),\n",
    "                CBR2d(out_channels, out_channels, kernel_size, padding, stride, bias)\n",
    "            )\n",
    "        \n",
    "        def PoolLayer2d(kernel_size=2, stride=2):\n",
    "            return nn.MaxPool2d(kernel_size=kernel_size, stride=stride)\n",
    "\n",
    "        def UpConvLayer2d(in_channels, out_channels, kernel_size=2, stride=2, padding=0):\n",
    "            return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "        self.encoder1 = ConvLayer2d(in_channels, 64)\n",
    "        self.pool1 = PoolLayer2d(2, 2)\n",
    "\n",
    "        self.encoder2 = ConvLayer2d(64, 128)\n",
    "        self.pool2 = PoolLayer2d(2, 2)\n",
    "\n",
    "        self.encoder3 = ConvLayer2d(128, 256)\n",
    "        self.pool3 = PoolLayer2d(2, 2)\n",
    "\n",
    "        self.encoder4 = ConvLayer2d(256, 512)\n",
    "        self.pool4 = PoolLayer2d(2, 2)\n",
    "\n",
    "        self.bridge = ConvLayer2d(512, 1024)\n",
    "\n",
    "        self.upconv4 = UpConvLayer2d(1024, 512)\n",
    "        self.decoder4 = ConvLayer2d(1024, 512)\n",
    "\n",
    "        self.upconv3 = UpConvLayer2d(512, 256)\n",
    "        self.decoder3 = ConvLayer2d(512, 256)\n",
    "\n",
    "        self.upconv2 = UpConvLayer2d(256, 128)\n",
    "        self.decoder2 = ConvLayer2d(256, 128)\n",
    "\n",
    "        self.upconv1 = UpConvLayer2d(128, 64)\n",
    "        self.decoder1 = ConvLayer2d(128, 64)\n",
    "\n",
    "        self.output_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.encoder1(x)\n",
    "        p1 = self.pool1(e1)\n",
    "\n",
    "        e2 = self.encoder2(p1)\n",
    "        p2 = self.pool2(e2)\n",
    "\n",
    "        e3 = self.encoder3(p2)\n",
    "        p3 = self.pool3(e3)\n",
    "\n",
    "        e4 = self.encoder4(p3)\n",
    "        p4 = self.pool4(e4)\n",
    "\n",
    "        bridge = self.bridge(p4)\n",
    "\n",
    "        u4 = self.upconv4(bridge)\n",
    "        c4 = torch.cat((u4, e4), dim=1)\n",
    "        d4 = self.decoder4(c4)\n",
    "\n",
    "        u3 = self.upconv3(d4)\n",
    "        c3 = torch.cat((u3, e3), dim=1)\n",
    "        d3 = self.decoder3(c3)\n",
    "\n",
    "        u2 = self.upconv2(d3)\n",
    "        c2 = torch.cat((u2, e2), dim=1)\n",
    "        d2 = self.decoder2(c2)\n",
    "\n",
    "        u1 = self.upconv1(d2)\n",
    "        c1 = torch.cat((u1, e1), dim=1)\n",
    "        d1 = self.decoder1(c1)\n",
    "\n",
    "        output = self.output_conv(d1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "in_channels = 3  # RGB 위성 사진\n",
    "out_channels = 9  # 9개 클래스\n",
    "\n",
    "# GPU가 사용 가능한지 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# 모델 생성 및 GPU로 이동\n",
    "model = UNet(in_channels=3, out_channels=9).to(device)\n",
    "\n",
    "all_masks = []\n",
    "for _, mask in dataset:\n",
    "    all_masks.append(mask.numpy().flatten())\n",
    "\n",
    "all_masks = np.concatenate(all_masks)\n",
    "unique, counts = np.unique(all_masks, return_counts=True)\n",
    "print(\"Class distribution in dataset:\")\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Class {u}: {c} pixels\")\n",
    "\n",
    "# 각 클래스의 가중치 계산\n",
    "class_weights = torch.tensor([1.0 / c for c in counts], dtype=torch.float).to(device)\n",
    "\n",
    "# 손실 함수에 가중치 적용\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 루프\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, masks in dataloader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.squeeze(1).to(device)  # 차원 줄이기\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader)}\")\n",
    "\n",
    "print(\"Training Finished.\")\n",
    "\n",
    "# 디렉토리 확인 및 생성\n",
    "save_dir = './../model_dict_save'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# 모델 저장\n",
    "torch.save(model.state_dict(), os.path.join(save_dir, 'unet_model4.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Class distribution in dataset:\n",
      "Class 0.03921568766236305: 3271738 pixels\n",
      "Class 0.0784313753247261: 435603 pixels\n",
      "Class 0.11764705926179886: 1972758 pixels\n",
      "Class 0.1568627506494522: 17130 pixels\n",
      "Class 0.19607843458652496: 482 pixels\n",
      "Class 0.23529411852359772: 31715 pixels\n",
      "Class 0.27450981736183167: 693109 pixels\n",
      "Class 0.3137255012989044: 181528 pixels\n",
      "Class 0.3921568691730499: 14367457 pixels\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "in_channels = 3  # RGB 위성 사진\n",
    "out_channels = 9  # 9개 클래스\n",
    "\n",
    "# GPU가 사용 가능한지 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# 모델 생성 및 GPU로 이동\n",
    "model = UNet(in_channels=3, out_channels=9).to(device)\n",
    "\n",
    "# 데이터셋에서 마스크 가져오기\n",
    "all_masks = []\n",
    "for _, mask in dataset:\n",
    "    all_masks.append(mask.numpy().flatten())\n",
    "\n",
    "all_masks = np.concatenate(all_masks)\n",
    "unique, counts = np.unique(all_masks, return_counts=True)\n",
    "print(\"Class distribution in dataset:\")\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Class {u}: {c} pixels\")\n",
    "\n",
    "# 각 클래스의 가중치 계산\n",
    "class_weights = torch.tensor([1.0 / c for c in counts], dtype=torch.float).to(device)\n",
    "\n",
    "# 손실 함수에 가중치 적용\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 루프\n",
    "num_epochs = 100\n",
    "save_dir = './../model_dict_save'\n",
    "\n",
    "# 디렉토리 확인 및 생성\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, masks in dataloader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.squeeze(1).to(device)  # 차원 줄이기\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader)}\")\n",
    "    \n",
    "    # 모델 저장\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f'unet_model_epoch_{epoch+1}.pth'))\n",
    "\n",
    "print(\"Training Finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디렉토리 확인 및 생성\n",
    "save_dir = './../model_dict_save'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# 모델 저장\n",
    "torch.save(model.state_dict(), os.path.join(save_dir, 'unet_model2.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 학습된 모델을 로드합니다 (이미 학습된 모델 파일이 있는 경우)\n",
    "model_path = './../model_dict_save/unet_model4.pth'\n",
    "model = UNet(in_channels=3, out_channels=9)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "# GPU가 사용 가능한지 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 입력 이미지 로드 및 전처리\n",
    "input_image_path = './test_image/LC_AP_37607047_021.tif'\n",
    "input_image = Image.open(input_image_path).convert('RGB')\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "input_tensor = transform(input_image).unsqueeze(0).to(device)  # 배치 차원 추가 및 GPU로 이동\n",
    "\n",
    "# 입력 이미지 시각화\n",
    "plt.imshow(np.transpose(input_tensor.squeeze(0).cpu().numpy(), (1, 2, 0)))\n",
    "plt.title(\"Input Image\")\n",
    "plt.show()\n",
    "\n",
    "# 모델 예측\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    output = torch.softmax(output, dim=1)  # 소프트맥스 적용\n",
    "    output = output.squeeze(0)  # 배치 차원 제거\n",
    "\n",
    "# 소프트맥스 적용 후 출력의 범위 확인\n",
    "print(f\"Output range: min={output.min().item()}, max={output.max().item()}\")\n",
    "\n",
    "# 예측 결과 확인\n",
    "predicted = torch.argmax(output, dim=0).cpu().numpy()  # 가장 높은 확률의 클래스 선택\n",
    "\n",
    "# 예측된 라벨 시각화\n",
    "plt.imshow(predicted, cmap='gray')\n",
    "plt.title(\"Predicted Label\")\n",
    "plt.show()\n",
    "\n",
    "# 각 클래스별 픽셀 수 확인\n",
    "unique, counts = np.unique(predicted, return_counts=True)\n",
    "print(\"Class distribution in predicted output:\")\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Class {u}: {c} pixels\")\n",
    "\n",
    "# 클래스 라벨을 그레이스케일 값으로 매핑\n",
    "label_to_color = {\n",
    "    0: 10, 1: 20, 2: 30, 3: 40, 4: 50, 5: 60, 6: 70, 7: 80, 8: 100\n",
    "}\n",
    "output_mapped = np.zeros_like(predicted, dtype=np.uint8)\n",
    "for k, v in label_to_color.items():\n",
    "    output_mapped[predicted == k] = v\n",
    "\n",
    "# 매핑된 결과 확인\n",
    "plt.imshow(output_mapped, cmap='gray')\n",
    "plt.title(\"Mapped Output\")\n",
    "plt.show()\n",
    "\n",
    "# 매핑된 결과를 이미지로 저장\n",
    "output_image = Image.fromarray(output_mapped)\n",
    "output_image_path = './test_image/labeled_image.tiff'\n",
    "output_image.save(output_image_path)\n",
    "\n",
    "print(\"라벨링 이미지가 저장되었습니다.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 샘플 이미지와 마스크 시각화\n",
    "sample_image, sample_mask = dataset[23]\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sample_image.permute(1, 2, 0))\n",
    "plt.title(\"Sample Image\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(sample_mask.squeeze(0), cmap='gray')\n",
    "plt.title(\"Sample Mask\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 모델 로드 및 장치로 이동\n",
    "model_path = './../model_dict_save/unet_model2.pth'\n",
    "model = UNet(in_channels=3, out_channels=9).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# 입력 이미지 배치 차원 추가 및 장치로 이동\n",
    "sample_image = sample_image.unsqueeze(0).to(device)\n",
    "\n",
    "# 모델 출력\n",
    "outputs = model(sample_image)\n",
    "\n",
    "# 소프트맥스 적용 후 클래스 예측\n",
    "outputs = torch.argmax(torch.softmax(outputs, dim=1), dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "# 예측 결과 시각화\n",
    "plt.imshow(outputs, cmap='gray')\n",
    "plt.title(\"Predicted Mask\")\n",
    "plt.show()\n",
    "\n",
    "# 실제 마스크와 예측 마스크 비교\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sample_mask.squeeze(0).cpu(), cmap='gray')\n",
    "plt.title(\"Actual Mask\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(outputs, cmap='gray')\n",
    "plt.title(\"Predicted Mask\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 샘플 이미지와 마스크 시각화\n",
    "sample_image, sample_mask = dataset[23]\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sample_image.permute(1, 2, 0))\n",
    "plt.title(\"Sample Image\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(sample_mask.squeeze(0), cmap='gray')\n",
    "plt.title(\"Sample Mask\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 모델 로드 및 장치로 이동\n",
    "model_path = './../model_dict_save/unet_model3.pth'\n",
    "model = UNet(in_channels=3, out_channels=9).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# 입력 이미지 배치 차원 추가 및 장치로 이동\n",
    "sample_image = sample_image.unsqueeze(0).to(device)\n",
    "sample_mask = sample_mask.unsqueeze(0).to(device)  # 필요에 따라 마스크도 이동\n",
    "\n",
    "# 모델 출력\n",
    "outputs = model(sample_image)\n",
    "\n",
    "# 출력 및 마스크 형태 확인\n",
    "print(\"Output shape:\", outputs.shape)\n",
    "print(\"Mask shape:\", sample_mask.shape)\n",
    "\n",
    "# 손실 계산을 위해 마스크의 차원 축소\n",
    "sample_mask = sample_mask.squeeze(1)  # [1, 1, 512, 512] -> [1, 512, 512]\n",
    "\n",
    "# 손실 함수\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 손실 계산\n",
    "loss = criterion(outputs, sample_mask.long())\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_masks = []\n",
    "for _, mask in dataset:\n",
    "    all_masks.append(mask.numpy().flatten())\n",
    "\n",
    "all_masks = np.concatenate(all_masks)\n",
    "unique, counts = np.unique(all_masks, return_counts=True)\n",
    "print(\"Class distribution in dataset:\")\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Class {u}: {c} pixels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 모델 로드 및 장치로 이동\n",
    "model_path = './../model_dict_save/unet_model2.pth'\n",
    "model = UNet(in_channels=3, out_channels=9).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# 샘플 이미지와 마스크 불러오기 및 전처리\n",
    "sample_image, sample_mask = dataset[23]\n",
    "sample_image = sample_image.unsqueeze(0).to(device)\n",
    "\n",
    "# 모델 출력\n",
    "with torch.no_grad():\n",
    "    outputs = model(sample_image)\n",
    "    print(\"Raw model output shape:\", outputs.shape)\n",
    "\n",
    "    # 소프트맥스 적용하여 각 클래스의 확률 계산\n",
    "    softmax_outputs = torch.softmax(outputs, dim=1)\n",
    "    print(\"Softmax applied output shape:\", softmax_outputs.shape)\n",
    "    print(\"Softmax output min:\", softmax_outputs.min().item())\n",
    "    print(\"Softmax output max:\", softmax_outputs.max().item())\n",
    "\n",
    "    # argmax 적용하여 각 픽셀의 최종 클래스를 선택\n",
    "    predicted_mask = torch.argmax(softmax_outputs, dim=1).squeeze().cpu().numpy()\n",
    "    print(\"Predicted mask shape:\", predicted_mask.shape)\n",
    "    print(\"Predicted mask unique values:\", np.unique(predicted_mask))\n",
    "\n",
    "# 예측 결과 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sample_mask.squeeze(0).cpu(), cmap='gray')\n",
    "plt.title(\"Actual Mask\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(predicted_mask, cmap='gray')\n",
    "plt.title(\"Predicted Mask\")\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
