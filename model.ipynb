{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from model.UNet import UNet\n",
        "from model.DeepLabV3Plus import DeepLabV3Plus\n",
        "from model.HRNet import HRNet\n",
        "\n",
        "import os\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SegmentationDataset(Dataset):\n",
        "  def __init__(self, data_type, mode, transform=None):    \n",
        "    self.mode = mode\n",
        "    self.transform = transform\n",
        "    self.input, self.label = [], []\n",
        "\n",
        "    # 작업할 폴더의 경로\n",
        "    path = './dataset/{0}s'.format(data_type) \n",
        "\n",
        "    self.input_path = os.path.join(path, self.mode, \"data\")\n",
        "    self.label_path = os.path.join(path, self.mode, \"label\")\n",
        "\n",
        "    # 폴더 안에 있는 데이터 파일들의 이름을 추출\n",
        "    data_names = [name.split('.')[0] for name in os.listdir(self.input_path)]\n",
        "\n",
        "    # 데이터 전처리\n",
        "    for data_name in data_names:\n",
        "        self.input.append('{0}/{1}.png'.format(self.input_path, data_name))\n",
        "        self.label.append('{0}/{1}.png'.format(self.label_path, data_name))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    input = Image.open(self.input[index])\n",
        "    label = Image.open(self.label[index])\n",
        "    \n",
        "    if self.transform:\n",
        "      input = self.transform(input)\n",
        "      label = self.transform(label)\n",
        "\n",
        "    return input, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset   = SegmentationDataset('building', 'train', transform)\n",
        "test_dataset    = SegmentationDataset('building', 'test',  transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 저장된 모델 가중치 로드\n",
        "#model.load_state_dict(torch.load(os.path.join(config[\"output_dir\"], config[\"model_name\"])) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_dir_path = \"./trained\"\n",
        "\n",
        "def train(model, dataset, num_epoch = 10):\n",
        "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True) \n",
        "\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters()) \n",
        "\n",
        "    for epoch in range(num_epoch):\n",
        "        model.train()\n",
        "\n",
        "        costs = []\n",
        "        \n",
        "        for batch in dataloader:\n",
        "            input, label = batch\n",
        "            \n",
        "            input = input.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            hypothesis = model(input)\n",
        " \n",
        "            cost = loss_func(hypothesis, label)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            cost.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            costs.append(cost.item())\n",
        "\n",
        "        #print(\"{0}/{1} : {2}\".format(epoch, num_epoch, np.mean(costs)))\n",
        "        \n",
        "        #torch.save(model.state_dict(), os.path.join(output_dir_path, \"{0}_epoch_{1:d}.pt\".format(type(model).__class__.__name__, epoch + 1)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Sizes of tensors must match except in dimension 1. Expected size 64 but got size 1 for tensor number 4 in the list.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m model2 \u001b[38;5;241m=\u001b[39m DeepLabV3Plus()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#model3 = HRNet().to(device)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#train(model1, train_dataset, num_epoch)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[6], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataset, num_epoch)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     18\u001b[0m label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 20\u001b[0m hypothesis \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m cost \u001b[38;5;241m=\u001b[39m loss_func(hypothesis, label)\n\u001b[0;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[1;32mc:\\Users\\hmud0\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\hmud0\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mk:\\KHJ Folder\\학교\\과제\\3학년 1학기\\오소프\\프로젝트\\Image-Segmentation\\model\\DeepLabV3Plus.py:102\u001b[0m, in \u001b[0;36mDeepLabV3Plus.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     99\u001b[0m e, low_level \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Atrous Spatial Pyramid Pooling\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m aspp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maspp\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Decoder\u001b[39;00m\n\u001b[0;32m    105\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(aspp, low_level)\n",
            "File \u001b[1;32mc:\\Users\\hmud0\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\hmud0\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mk:\\KHJ Folder\\학교\\과제\\3학년 1학기\\오소프\\프로젝트\\Image-Segmentation\\model\\DeepLabV3Plus.py:42\u001b[0m, in \u001b[0;36mASPP.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     38\u001b[0m aspp4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maspp4(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m     40\u001b[0m pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m cat \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43maspp1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maspp2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maspp3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maspp4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# 1x1 Convolution 적용\u001b[39;00m\n\u001b[0;32m     45\u001b[0m conv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(cat)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 64 but got size 1 for tensor number 4 in the list."
          ]
        }
      ],
      "source": [
        "num_epoch = 10\n",
        "\n",
        "#model1 = UNet().to(device)\n",
        "model2 = DeepLabV3Plus().to(device)\n",
        "#model3 = HRNet().to(device)\n",
        "\n",
        "#train(model1, train_dataset, num_epoch)\n",
        "train(model2, train_dataset, num_epoch)\n",
        "#train(model3, train_dataset, num_epoch)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
